<!DOCTYPE html>
<html lang="en">

<head>

    <!-- Meta Info for Google Search -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Yang Weng">
    <meta property="og:description" content="Associate Professor of ECEE at Arizona State University">
    <meta property="og:image" content="https://yweng3.github.io/Image/Photos/Yang.jpg">
    <meta property="og:url" content="https://yweng3.github.io">
    <meta name="author" content="Yang Weng">

    <title>Webinar</title>

    <!--dependencies-->
    <link rel="stylesheet" href="../../CSS/homepage.css">
    <link rel="stylesheet" href="../../CSS/webinar.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/all.min.css"    integrity="sha512-BnbUDfEUfV0Slx6TunuB042k9tuKe3xrD6q4mg5Ed72LTgzDIcLPxg6yI2gcMFRyomt+yJJxE+zJwNmxki6/RA=="    crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="../../CSS/index.js" type="module"></script>
    <style type="text/css">
        a{color:black;}
    </style>
</head>

<body>

    <!-- Entire Page -->
    <div class="row">

        <!-- Top-bar Menu -->
            <div id="menu-placeholder"></div>
            <script>
            fetch('../../menu.html')
                .then(response => response.text())
                .then(data => {
                document.getElementById('menu-placeholder').innerHTML = data;
                });
            </script>
        <!-- Top Big Title -->
        <div class="bigtitle" id="bigtitle">
            <div class="thetitle">Webinar</div>
        </div>

        <!-- Webinars -->
        <main class="container">
            <div class="column">

            <div class="web-individual">
                <div class="web">
                    <div class="speaker">
                    <div class="speaker-content">
                        <div class="speaker-photo" style="background-image:url(../../Image/Webinar/1KJMZcGNw3d1DNiEutlLMRU3QkzhjuoNq.png);"></div>
                        <div class="speaker-name">Yize Chen</div>
                        <div class="speaker-school">University of Alberta</div>
                    </div>
                    <div class="speaker-content">
                        <div class="speaker-photo" style="background-image:url(../../Image/Webinar/1ALSZ815dAoJnyGFTsI1gIKg0XVZDzKPo.png);"></div>
                        <div class="speaker-name">Yuzhuo Li</div>
                        <div class="speaker-school">University of Alberta</div>
                    </div>
                    <div class="speaker-content">
                    <div class="talktitle">The Unseen AI Disruptions for Power Grids: LLM-Induced Transients</div>
                        <p><b>Date</b>:&nbsp;&nbsp;September 27, 2024</p>
                        <p><b>Speaker</b>:&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/author/37086364057">Yize's website &nbsp;&nbsp;</a><a href="https://ieeexplore.ieee.org/author/37086197846">Yuzhuo's website &nbsp;&nbsp;</a></p>
                        <p><b>Contact</b>:&nbsp;&nbsp;yizechen@ust.hk;yuzhuo@ualberta.ca</p>
                        <p><b>Material</b>:&nbsp;&nbsp;<a class="flyer" href="https://drive.google.com/uc?export=view&id=1Ao8UkntmMy-TCx7Pls6-aJTtpOlcPcBe"><span>Flyer</span></a>&nbsp;&nbsp;<a class="flyer" href="https://drive.google.com/uc?export=view&id=1ztOaBRqTP3nlmNAfvlWyf4DxGpNkfaA8"><span>Slides</span></a></p>
                   </div>
                </div>
                </div>
                <br>
                <br>
                <div class="bios">
                    <b>Bios</b>
                    <p>Yize Chen is an assistant professor at the Department of Electrical and Computer Engineering at the University of Alberta, Canada. Yize’s research focuses on the intersection between control, optimization and machine learning, and he is interested in designing cyber-physical systems, especially power systems with performance guarantees. He is also committed to achieving sustainable and autonomous clean energy systems. Previously, he got his Ph.D. degree in Electrical and Computer Engineering from University of Washington in 2021; and his undergraduate degrees in Automation from Chu Kochen College at Zhejiang University in 2016. From 2022-2023, he was an assistant professor at AI Thrust, Information Hub at Hong Kong University of Science and Technology, also affiliated with the Department of Computer Science and Engineering at HKUST. Before that, he was a postdoctoral researcher at the Computing Sciences Area of Lawrence Berkeley National Lab. He has also held research positions at ISO New England, Microsoft Research, Los Alamos National Laboratory, and Harvard Medical School, working on a set of problems related to data centers, power grid infrastructures, biological dynamics and the built environment. He is also a recipient of several best paper and prize paper awards at IEEE PES General Meeting (2024, 2022), Power Systems Computation Conference (PSCC) (2020), and ACM e-Energy (2019).;Yuzhuo Li is currently a Postdoctoral Fellow at the University of Alberta. He received the B.S. and M.S. degrees in control science and engineering from Shandong University, Jinan, China, in 2012 and 2015, respectively, and the Ph.D. degree in energy systems from the University of Alberta, Edmonton, Canada, in 2021. His main research interests include intelligent and graph-based systematic power electronics design and electrical systems.
                    </p>
                </div>
                <div class="bios">
                    <b>Abstract</b>
                    <p>Recent breakthroughs of large language models (LLMs) have exhibited superior capability across major industries and stimulated multi-hundred-billion-dollar investment in AI-centric data centers in the next 3-5 years. This, in turn, bring the increasing concerns on sustainability and AI-related energy usage. However, with fast, transient dynamics, AI infrastructure features ultra-low inertia, sharp power surge and dip, and a significant peak-idle power ratio. The power scale covers from several hundred watts to megawatts, even to gigawatts. These never-seen-before characteristics make AI a very unique load and pose threats to the power grid reliability and resilience. To reveal this hidden problem, this talk examines the scale of AI power consumption, analyzes AI transient behaviour in various scenarios, develops high-level mathematical models to depict AI workload behaviour and discusses the multifaceted challenges and opportunities they potentially bring to existing power grids. Observing the rapidly evolving machine learning (ML) and AI technologies, this work emphasizes the critical need for interdisciplinary approaches to ensure reliable and sustainable AI infrastructure development, and provides a starting point for researchers and practitioners to tackle such challenges.
                    </p>
                </div>
                    <div class="bios">
                        <b>Talk Outline</b><br>
                        <span>Background</span>
<li>Overview of Artificial Intelligence (AI) workloads and their growing impact.</li>
<li>The relevance of understanding power consumption in AI workloads.</li>
<li>Introduction to the relationship between AI systems, hardware, and energy demand.</li>
<li>Importance of optimizing power consumption for sustainable AI development.</li>
<br>
<span>Characteristics of AI Loads</span>
<li>Distinguishing features of AI workloads (e.g., computation-intensity, memory usage).</li>
<li>Key AI workload types: training vs. inference.</li>
<li>Variability in workload demands based on model architecture (e.g., transformers, CNNs).</li>
<li>Hardware-software interplay in influencing load characteristics.</li>
<br>
<span>Power Consumption Modeling of AI Workloads</span>
<li>Methods for modeling power consumption in AI workloads.</li>
<li>Empirical approaches: data-driven analysis.</li>
<li>Analytical models based on computational and memory usage.</li>
<li>Challenges in accurately modeling power usage due to workload heterogeneity.</li>
<li>Tools and frameworks used for AI power consumption modeling.</li>
<br>
<span>AI Power Consumption Analysis: Case Studies</span>
<li>Real-world examples of AI workload power consumption.</li>
<li>Energy consumption in large-scale training (e.g., GPT-3, BERT).</li>
<li>Case studies from industry: cloud-based vs. on-premise workloads.</li>
<li>Comparative analysis of different hardware platforms (e.g., GPUs, TPUs, CPUs).</li>
<li>Insights into energy efficiency trends across AI models and infrastructure.</li>
<br>
<span>Future Research Directions</span>
<li>Innovations in hardware design for AI power efficiency.</li>
<li>Role of software optimization in reducing energy costs (e.g., sparsity, quantization).</li>
<li>Incorporating renewable energy sources in AI computing facilities.</li>
<li>Opportunities for interdisciplinary research (e.g., AI and sustainability).</li>
<li>Policy and governance frameworks for AI energy regulation.</li>
<br>
<span>Conclusions</span>
<li>Summary of key findings and takeaways.</li>
<li>Importance of addressing AI's energy consumption for long-term viability.</li>
<li>Call to action for collaboration across academia, industry, and policymakers.</li>
<li>Future outlook: balancing AI innovation with environmental sustainability.</li>
<br>

                        
                    </div>
                <div class="materials">
                <div class="bios">
                    <b>Video</b><br>
                    <iframe width="700" height="540" src="https://www.youtube.com/embed/1VGRMDNgDCI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                </div>
<br>                    <div class="bios">
                        <b>Q & A</b>
                        <p><strong>Q</strong>: Does the dynamic load variability really interrupt the power grid at the second level, and could this problem be resolved by data center-side cheap short-term storage like supercapacitors?<br>
<strong>A</strong>: Yes, it is possible to resolve this issue using supercapacitors or short-term storage. However, before implementing a solution, it is important to understand the transient behavior of the data center load and the AI load specifically. Research shows that utility companies, such as those working with Meta (Facebook), have reported fluctuations due to AI model training. These fluctuations are a real issue and could become more significant in the future.<br>
<br>
<strong>Q</strong>: Does the variability of data center power consumption aggregate to be smoother at a larger scale, or is it still significant?<br>
<strong>A</strong>: The answer depends on the nature of the load. If the load consistently hits GPU power limits, the power curve becomes flat. However, transient behavior depends on user activity, such as simultaneous prompts during events. Aggregation at a larger scale might smooth the curve, but power fluctuations due to synchronized GPU operations still occur. This is particularly noticeable during model training, which can cause significant power dips.<br>
<br>
<strong>Q</strong>: How do you calculate the W coefficients (related to power consumption)?<br>
<strong>A</strong>: There is no specific method currently to calculate this. For training, peak power and checkpoint timing can be controlled. However, inference depends on user behavior, which introduces stochasticity. Interruptions during training, such as unplanned stops, create immediate power consumption drops. Future work might focus on detailed calculations per training step to build more accurate equations.<br>
<br>
<strong>Q</strong>: Are there weekly or monthly behaviors in AI load? What types of resources can be coupled with AI training consumption?<br>
<strong>A</strong>: Training behavior is algorithm-determined and can exhibit certain periodicities. For instance, checkpointing every 5 minutes introduces transient power dips. Inference, influenced by user behavior, shows workday or daily peaks similar to EV load patterns. Coupling with renewables is possible if renewable generation is predictable. However, some companies prefer stable sources like nuclear power to avoid renewables' variability.<br>
<br>
<strong>Q</strong>: How does aggregated data center consumption at a substation level compare to individual experiments? Are the power consumption numbers from your hardware setup?<br>
<strong>A</strong>: Aggregated consumption might smooth the curve, but synchronized GPU operations still produce transient effects. The power consumption data shown is based on our hardware setup, where peak values were measured under specific conditions.<br>
<br>
<strong>Q</strong>: Did you collect these data sets privately or are they publicly available?<br>
<strong>A</strong>: The MIT Supercloud dataset is publicly available. It includes GPU power consumption data, but significant cleaning and transformation were needed for analysis.<br>
<br>
<strong>Q</strong>: Does the GPU power consumption include auxiliary components like fans, and how is it measured?<br>
<strong>A</strong>: The GPU power consumption is measured using Nvidia’s NVSMI tool, which extracts power data directly from the motherboard. Auxiliary components like fans may be included depending on the tool's definition. Future work aims to capture higher-resolution data at the hardware level for more precise measurements.<br>
<br>
<strong>Q</strong>: What are your thoughts on the potential balance of using small GPT tasks to capture large training loads?<br>
<strong>A</strong>: This is part of ongoing work. Current efforts aim to scale up simulations and provide a more comprehensive understanding by using larger GPU clusters and data centers.<br>
<br>

                        </p>
                    </div>
                    </div>
            </div>
            </div>
        </main>
    </div>
</body>

<script type="text/javascript">
    document.getElementById("menu-webinar").classList.add('current');
</script>

</html>